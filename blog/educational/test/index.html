<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#1b1b1b">
	<title>Test | Alireza&#39;s Mindmap</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="ENTER DESCRIPTION">
		<meta property="og:title" content="Test" />
<meta property="og:description" content="ENTER DESCRIPTION" />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/educational/test/" /><meta property="article:section" content="educational" />
<meta property="article:published_time" content="2024-04-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-04-08T00:00:00+00:00" />

		<meta itemprop="name" content="Test">
<meta itemprop="description" content="ENTER DESCRIPTION"><meta itemprop="datePublished" content="2024-04-08T00:00:00+00:00" />
<meta itemprop="dateModified" content="2024-04-08T00:00:00+00:00" />
<meta itemprop="wordCount" content="7861">
<meta itemprop="keywords" content="tag1,tag2," />
	<link rel="stylesheet" href="/css/bundle.css">
	<link rel="stylesheet" href="/css/themes/dark-violet.css">
	<link rel="icon" href="/icons/16.png" sizes="16x16" type="image/png">
	<link rel="icon" href="/icons/32.png" sizes="32x32" type="image/png">
	<link rel="manifest" href="/manifest.json">
</head>
<body class="body kind-page">
	<header class="header">
	<a class="logo" href="/">Alireza&#39;s Mindmap</a>
	
<nav class="main-nav main-nav--right" role="navigation">
	<button id="toggle" class="main-nav__btn" aria-label="Menu toggle" aria-expanded="false" tabindex="0">
		<div class="main-nav__btn-box" tabindex="-1">
			<svg class="main-nav__icon icon-menu" width="18" height="18" viewBox="0 0 18 18">
				<path class="icon-menu__burger" d="M18 0v3.6H0V0h18zM0 10.8h18V7.2H0v3.6zM0 18h18v-3.6H0V18z"/>
				<path class="icon-menu__x" d="M11.55 9L18 15.45 15.45 18 9 11.55 2.55 18 0 15.45 6.45 9 0 2.55 2.55 0 9 6.45 15.45 0 18 2.55 11.55 9z"/>
			</svg>
		</div>
	</button>
	<ul id="menu" class="main-nav__list">
			<li class="main-nav__item main-nav__item--active">
				
					
					<span class="main-nav__text">Test</span>
					
				
			</li>
	</ul>
</nav>
</header>
	<div class="primary">
	
	<main class="main">
		
<nav class="breadcrumb block" aria-label="breadcrumb">
	<ol class="breadcrumb__list">
		
		<li class="breadcrumb__item">
			<a class="breadcrumbs__link" href="/">Home</a>
		</li>
		<li class="breadcrumb__item">
			<a class="breadcrumbs__link" href="/educational/">Educationals</a>
		</li>
		<li class="breadcrumbs__item breadcrumb__item--active" aria-current="page">Test</li>
	</ol>
</nav>
		<div class="single block">
			<article class="entry">
	<div class="entry__meta meta mb">
	<time class="entry__meta-published meta-published" datetime="2024-04-08T00:00:00Z">April 08, 2024</time>
<div class="entry__meta-categories meta-categories">
	<span class="meta-categories__list">Categories:
		<a class="meta-categories__link" href="/categories/cat1/" rel="category">cat1</a>, 
		<a class="meta-categories__link" href="/categories/cat2/" rel="category">cat2</a>
	</span>
</div>
<div class="entry__meta-tags meta-tags">
	<span class="meta-tags__list">Tags:
		<a class="meta-tags__link" href="/tags/tag1/" rel="tag">tag1</a>, 
		<a class="meta-tags__link" href="/tags/tag2/" rel="tag">tag2</a>
	</span>
</div>
	</div>
				<h1 class="entry__title">Test</h1>
<details class="entry__toc toc" open>
	<summary class="toc__title">Table of Contents</summary>
	<nav id="TableOfContents">
  <ul>
    <li><a href="#video-codecs---intro-and-basic-concepts">Video Codecs - Intro and Basic Concepts</a>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#short-history">Short History</a></li>
        <li><a href="#general-concepts--definitions">General Concepts &amp; Definitions</a>
          <ul>
            <li><a href="#codec">CODEC</a></li>
            <li><a href="#spatial-and-temporal-dimensions">Spatial and Temporal Dimensions</a></li>
            <li><a href="#color-model-color-space">Color Model (Color Space)</a>
              <ul>
                <li><a href="#semantics">Semantics</a></li>
                <li><a href="#chroma-subsampling--chroma-downsampling">Chroma Subsampling / Chroma Downsampling</a></li>
                <li><a href="#subtypes-and-formats-pixel-formats">Subtypes and Formats (Pixel Formats)</a></li>
              </ul>
            </li>
            <li><a href="#endianness">Endianness</a></li>
            <li><a href="#entropy-coding">Entropy Coding</a></li>
          </ul>
        </li>
        <li><a href="#transforms-and-quantization">Transforms and Quantization</a>
          <ul>
            <li><a href="#transforms">Transforms</a>
              <ul>
                <li><a href="#block-based-transforms">Block Based Transforms</a></li>
                <li><a href="#image-based-transforms">Image Based Transforms</a></li>
              </ul>
            </li>
            <li><a href="#quantization">Quantization</a>
              <ul>
                <li><a href="#scalar-quantization">Scalar Quantization</a></li>
                <li><a href="#vector-quantization">Vector Quantization</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#general-video-compression-concepts">General Video Compression Concepts</a>
          <ul>
            <li><a href="#samples">Samples</a></li>
            <li><a href="#blocks">Blocks</a></li>
            <li><a href="#macroblocks">Macroblocks</a>
              <ul>
                <li><a href="#sub-macroblocks">Sub Macroblocks</a></li>
              </ul>
            </li>
            <li><a href="#slices">Slices</a>
              <ul>
                <li><a href="#intra-coded-slice-i-slice">Intra-coded Slice (I-Slice)</a></li>
                <li><a href="#predictive-slice-p-slice">Predictive Slice (P-Slice)</a></li>
                <li><a href="#bipredictive-slice-b-slice">Bipredictive Slice (B-Slice)</a></li>
                <li><a href="#switching-i-slice-si-slice">Switching I Slice (SI-Slice)</a></li>
                <li><a href="#switching-p-slice-sp-slice">Switching P Slice (SP-Slice)</a></li>
                <li><a href="#instantaneous-decoding-refresh-slice-idr-slice">Instantaneous Decoding Refresh Slice (IDR-Slice)</a></li>
              </ul>
            </li>
            <li><a href="#pictures-frames-or-fields">Pictures (Frames or Fields)</a>
              <ul>
                <li><a href="#frame-types">Frame Types</a></li>
                <li><a href="#field-types">Field Types</a></li>
              </ul>
            </li>
            <li><a href="#group-of-pictures-gop">Group of Pictures (GOP)</a>
              <ul>
                <li><a href="#open-gop">Open GOP</a></li>
                <li><a href="#closed-gop">Closed GOP</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#compression-artifacts">Compression Artifacts</a>
          <ul>
            <li><a href="#spatial-artifacts">Spatial Artifacts</a>
              <ul>
                <li><a href="#blurring">Blurring</a></li>
                <li><a href="#blocking">Blocking</a></li>
                <li><a href="#ringing">Ringing</a></li>
                <li><a href="#basis-pattern">Basis Pattern</a></li>
                <li><a href="#color-bleeding">Color Bleeding</a></li>
              </ul>
            </li>
            <li><a href="#temporal-artifacts">Temporal Artifacts</a>
              <ul>
                <li><a href="#flickering">Flickering</a></li>
                <li><a href="#jerkiness">Jerkiness</a></li>
                <li><a href="#floating">Floating</a></li>
              </ul>
            </li>
            <li><a href="#other-image-artifacts">Other Image Artifacts</a></li>
            <li><a href="#interlacing-artifacts">Interlacing Artifacts</a></li>
            <li><a href="#aliasing">Aliasing</a></li>
          </ul>
        </li>
        <li><a href="#image-quality-assessment">Image Quality Assessment</a>
          <ul>
            <li><a href="#psnr">PSNR</a></li>
            <li><a href="#ssim">SSIM</a></li>
            <li><a href="#ms-ssim">MS-SSIM</a></li>
            <li><a href="#vmaf">VMAF</a></li>
            <li><a href="#example">Example</a></li>
          </ul>
        </li>
        <li><a href="#sources">Sources</a></li>
      </ul>
    </li>
  </ul>
</nav>
</details>
				<div class="entry__content"><h2 id="video-codecs---intro-and-basic-concepts">Video Codecs - Intro and Basic Concepts</h2>
<p>General concepts applicable to all discussions about video in general. It can act as a reference for future documents. It is mostly an aggregation of different topics from different sources, with additions to make it easier to understand for me personally.</p>
<p>I have tried to link to the sources as much as I could, but I may have missed something. In that case, please contact me :D</p>
<h3 id="introduction">Introduction</h3>
<p>Video is a time-ordered sequence of frames, or images. A frame is a 2D buffer of pixels (spatial dimension), and the time aspect denotes a temporal dimension.</p>
<p>Modern LCD and LED displays represent images by their Red, Green and Blue colors (RGB). You might have heard the term <em>8-bit</em> color, which means each color component can have 8 bits or $2^8=256$ possible values.</p>
<p>A <em>byte</em> is defined as 8 bits of data; therefore, in this case, each pixel has 3 bytes, for the red, green and blue components. We can do some basic calculations for the bytes required to store a raw 60 frames per second 1080p video:</p>
<p>$$
\begin{aligned}
&amp;\underset{\text{frames per second}}{60} \times \underset{\text{pixels per frame}}{1920 \times 1080} \times \underset{\text{bytes per pixel}}{3}
\
&amp;= 373248000 \text{ Bytes/s} \underset{\text{divide by 1E6}}{=} 373.248 \text{ MegaBytes/s}
\end{aligned}
$$</p>
<p>Which is a lot! Even USB 2.0 only supported at most 60 MB/s. We can see now why compression is so important. Proper algorithms can give us the same perceptual quality at a fraction of the bytes.</p>
<h3 id="short-history">Short History</h3>
<p>Three groups have been consistently involved in video compression. ITU-T, ISO and IEC. In recent years, the push for royalty-free options has introduced more choices, but this doesn&rsquo;t mean the patented stuff will be discarded anytime soon.<br>
For example, even H.264, which is getting pretty old at this point, is still being used by about 80% of video industry developers as of 2023 (down from about 90% in 2019 - source: <a href="https://bitmovin.com/video-developer-report">bitmovin</a>).<br>
The schemes provided by these entities are usually just describing the compressed bitstream and providing some form of reference software for encoding/decoding. More performant variants and hardware encoding/decoding (using the GPU) are usually developed later on.<br>
Here is a basic timeline of the most common compression schemes in use today (source: mostly Wikipedia):</p>
<table>
<thead>
<tr>
<th style="text-align:center">Year/Author</th>
<th style="text-align:center">ITU-T</th>
<th style="text-align:center">ISO/IEC</th>
<th style="text-align:center">ITU-T + ISO/IEC</th>
<th style="text-align:center">Google</th>
<th style="text-align:center">AOMedia</th>
<th style="text-align:center">Name</th>
<th style="text-align:center">Desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1989</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">JPEG</td>
<td style="text-align:center">Joint Photographic Experts Group (JPEG) joint committee between ISO/IEC and ITU-T - release 1989 - still maintained</td>
</tr>
<tr>
<td style="text-align:center">1991</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">MPEG-1</td>
<td style="text-align:center">Mp layer 3 (mp3) still used today!</td>
</tr>
<tr>
<td style="text-align:center">1993</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">H.261</td>
<td style="text-align:center">First truly practical digital video coding standard, essentially obsolete now</td>
</tr>
<tr>
<td style="text-align:center">1996</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">H.263</td>
<td style="text-align:center">Basis for the development of MPEG-4 Part 2</td>
</tr>
<tr>
<td style="text-align:center">1996</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">MPEG-2</td>
<td style="text-align:center">Still widely used</td>
</tr>
<tr>
<td style="text-align:center">1998</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">MPEG-4</td>
<td style="text-align:center">Still maintained</td>
</tr>
<tr>
<td style="text-align:center">2002</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">MPEG-7</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">2004</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">H.264 (AVC)</td>
<td style="text-align:center">Still widely used</td>
</tr>
<tr>
<td style="text-align:center">2008</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">VP8</td>
<td style="text-align:center">First attempt at royalty-free, ideas from H.263,4</td>
</tr>
<tr>
<td style="text-align:center">2013</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">H.265 (HEVC)</td>
<td style="text-align:center">Up to 50% better compression than AVC</td>
</tr>
<tr>
<td style="text-align:center">2013</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">VP9</td>
<td style="text-align:center">Comparable to H.265, performance not much better, partially patented</td>
</tr>
<tr>
<td style="text-align:center">2018</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center">AV1</td>
<td style="text-align:center">Used elements from Daala (Xiph.org), Thor (Cisco Systems), and VP10 (Google), royalty-free, resource heavy but good compression</td>
</tr>
<tr>
<td style="text-align:center">2020</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">H.266 (VVC)</td>
<td style="text-align:center">Up to 50% better compression than HEVC</td>
</tr>
</tbody>
</table>
<p>Methods in use by each new scheme is motivated by the predecessors, and aims to build on what already exists. This makes each iteration better, but also means learning the latest technologies more difficult, as there is a lot to catch up on.<br>
Of course, this document doesn&rsquo;t even go into the compression methods used at all. Those are coming later :D</p>
<h3 id="general-concepts--definitions">General Concepts &amp; Definitions</h3>
<p>Some commonly used concepts, terms and phrases in video engineering. These don&rsquo;t have to do with video per se, but come up very often in this field.</p>
<h4 id="codec">CODEC</h4>
<p>Created from the words en<strong>CO</strong>der and <strong>DEC</strong>oder. They describe methods for encoding (compressing) and decoding (decompressing) media.</p>
<h4 id="spatial-and-temporal-dimensions">Spatial and Temporal Dimensions</h4>
<p>If we think of an image as a 2D array, a video is 3-dimensional data. The 2 dimensions of the image are called <strong>spacial dimensions</strong>, while the third dimension, representing the flow of time, is called the <strong>temporal dimension.</strong><br>
Video compression aims to make use of compression in both dimensions.</p>
<h4 id="color-model-color-space">Color Model (Color Space)</h4>
<p><a href="https://learn.microsoft.com/en-us/windows/win32/medfound/about-yuv-video">Source</a><br>
Different ways of representing a conventional colored image. Some, like the previously mentioned RGB model, split an image into red, green and blue components, which is usually how images are displayed (Red, Blue and Green pixels).</p>
<p>The Human Visual System (HVS) is less sensitive to color than to the luminance (black and white information). This means discarding some color information should have less noticeable impact to the viewer, while giving a potentially big boost to compression efficiency. To extract color from images, alternative <strong>color spaces</strong> are used. Color spaces do something similar to [[#Transforms]] that will be discussed later; they describe the same RGB information in another way, such that the grayscale data is separate from color information. One such method, the $Y \prime C_rC_b$ color space, is a popular way of doing so. Only the grayscale component and two of the three color components are needed to fully represent the RGB values without loss. The chosen color components are usually the red ($C_r$) and blue ($C_b$) components, and $Y\prime$ is the grayscale component.</p>
<p>$$
\begin{equation}
\begin{gathered}
Y  \prime= k_rR + k_g G + k_b B \
C_r = R - Y \
C_g = G - Y \
C_b = B - Y
\end{gathered}
\end {equation}
$$</p>
<p>Color spaces make discarding color information much easier, as the components are now separate. Note that color spaces also included a separate alpha component, which is just the transparency so it isn&rsquo;t covered in this section.</p>
<h5 id="semantics">Semantics</h5>
<p>Source: WikiPedia</p>
<p>The term used so far has been $Y \prime C_rC_b$, but often you will see the term YUV instead. The term YUV refers to an analog TV encoding scheme, while $Y \prime C_rC_b$ refers to a digital encoding scheme. One difference between the two is that the scale factors on the chroma components (U, V, $C_b$, and $C_r$) are different. However, the term YUV is often used erroneously to refer to $Y \prime C_rC_b$ encoding. Hence, expressions like &ldquo;4:2:2 YUV&rdquo; always refer to 4:2:2 $Y \prime C_rC_b$, since there simply is no such thing as 4:x:x in analog encoding (such as YUV). Pixel formats used in $Y \prime C_rC_b$ can be referred to as YUV too, for example <code>yuv420p</code>, <code>yuvj420p</code> and many others.</p>
<p>In a similar vein, the term luminance and the symbol Y are often used erroneously to refer to luma, which is denoted with the symbol $Y\prime$. Note that the luma ($Y\prime$) of video engineering deviates from the luminance (Y) of color science (as defined by CIE). Luma is formed as the weighted sum of gamma-corrected (tristimulus) RGB components. Luminance is formed as a weighed sum of linear (tristimulus) RGB components. In practice, the CIE symbol Y is often incorrectly used to denote luma. Note that the prime symbol &rsquo; is used to indicate gamma correction.</p>
<p>Similarly, the chroma of video engineering differs from the chrominance of color science. The chroma of video engineering is formed from weighted tristimulus components (gamma corrected, OETF), not linear components. In video engineering practice, the terms chroma, chrominance, and saturation are often used interchangeably to refer to chroma, but it is not a good practice.</p>
<p><strong>In the rest of this document, $Y \prime C_rC_b$ and YUV are used interchangeably</strong> and the <strong>color</strong> and <strong>grayscale</strong> components will be referred to as <strong>chroma</strong> and <strong>luma</strong> respectively.</p>
<h5 id="chroma-subsampling--chroma-downsampling">Chroma Subsampling / Chroma Downsampling</h5>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Chroma_subsampling">Image Source</a><br>
As discussed before, because the HVS is less sensitive to color than grayscale, the chroma components may be represented with a lower resolution than the luma component. This processed is called chroma subsampling or chroma downsampling.</li>
</ul>
<p>There are many sampling systems and ratios, but usually they are annotated by numbers in $J:b:c:\text{alpha}$ format, where:<br>
$J$: width of pixels where this subsampling method applied to<br>
$a$: number of chroma sample changes within first row<br>
$b$: number of chroma sample changes in second row, relative to first row<br>
![[Pasted image 20240407165341.png]]</p>
<p>Popular chroma subsampling methods:</p>
<ul>
<li>4:4:4 - Preserves the full fidelity of the chrominance components (equivalent of the original image)</li>
<li>4:2:2 - Chroma components have the same vertical resolution as the luma but half the horizontal resolution</li>
<li>4:2:0 - Chroma components each have half the horizontal and vertical resolution of luma</li>
</ul>
<h5 id="subtypes-and-formats-pixel-formats">Subtypes and Formats (Pixel Formats)</h5>
<ul>
<li><a href="https://fourcc.org/yuv.php">Source 1</a></li>
<li><a href="https://gist.github.com/Jim-Bar/3cbba684a71d1a9d468a6711a6eddbeb">Source 2</a></li>
<li><a href="https://en.wikipedia.org/wiki/Color_depth">Source 3</a></li>
</ul>
<p>Some important details that still need to be addressed about the $Y \prime C_rC_b$ components are:</p>
<ul>
<li>How many bits do we need to represent each component? Referred to as <strong>Bit Depth</strong></li>
<li>How are these components placed together and stored? Referred to as <strong>Pixel Layout</strong></li>
</ul>
<p>There are many different subtypes of $Y \prime C_rC_b$, across different standards and specifications. I&rsquo;m not sure exactly why there are just so many, but it could be a combination of different developing bodies, more efficiency or better memory management when processing/storing some subtypes over the others, etc.<br>
This doesn&rsquo;t only apply to $Y \prime C_rC_b$ Even RGB can be represented in many different ways when digitalized.</p>
<h6 id="bit-depth--bpp-bits-per-pixel">Bit Depth / BPP (Bits Per Pixel)</h6>
<p>(Describes how many bits are needed to represent each component)<br>
As many as you want! Currently, 32-bit color is pretty common. 32 bit here means 8 bits for each of the R, G, B and alpha values (alpha refers to transparency). Below, you can see the difference between 4, and 8 bits-per-component in an image (image source: WikiPedia).</p>
<p>![[Pasted image 20240401191959.png||200x200]] ![[Pasted image 20240401192009.png|200x200]]</p>
<p>Nowadays, it&rsquo;s not uncommon to see 10, 12 or 16 bit per component in images and video.</p>
<h6 id="pixel-format--pixel-layout">Pixel Format / Pixel Layout</h6>
<p>(Describes how components are stored digitally)<br>
There are 3 approaches to take when placing chroma (Y) and chroma (U, V) components in a bitstream.</p>
<ul>
<li><strong>Packed</strong> / <strong>interleaved</strong>: Mixed in a single array. Pixels are organized into groups, whose layout depends on the format</li>
<li><strong>Semi-Packed</strong> (often grouped together with the first): Two components are mixed, the other is separate</li>
<li><strong>Planar</strong>: All components are stored separately</li>
</ul>
<h6 id="examples">Examples</h6>
<p><a href="https://gstreamer.freedesktop.org/documentation/additional/design/mediatype-video-raw.html">Source</a><br>
An excellent source for examples is the GStreamer documentation. Below, three very common schemes are discussed.</p>
<h6 id="y444">Y444</h6>
<p>Planar 4:4:4 YUV</p>
<pre tabindex="0"><code>        Component 0: Y
          depth:           8
          pstride:         1
          default offset:  0
          default rstride: RU4 (width)
          default size:    rstride (component0) * height

        Component 1: U
          depth            8
          pstride:         1
          default offset:  size (component0)
          default rstride: RU4 (width)
          default size:    rstride (component1) * height

        Component 2: V
          depth:           8
          pstride:         1
          default offset:  offset (component1) + size (component1)
          default rstride: RU4 (width)
          default size:    rstride (component2) * height

        Image
          default size: size (component0) +
                        size (component1) +
                        size (component2)
</code></pre><p>Let&rsquo;s digest:</p>
<ul>
<li>The <code>depth</code> is 8 for all components, this is the bit depth concept discussed earlier</li>
<li><code>pstride</code> refers to the distance between two of the same component. Since it is planar, they are all 1</li>
<li><code>offset</code> refers to the starting position of each component in the bitstream (see below example)</li>
<li><code>rstride</code> refers to the row length; a 2D image is effectively a single array, with each row coming after the other. The <code>RU4</code> function rounds up to the first multiple of 4. This is done for memory alignment reasons; even if the whole row isn&rsquo;t used, it is still allocated this way.</li>
<li><code>size</code>: obviously refers to the total memory size of the image.<br>
Let&rsquo;s assume we need to store a 2x2 pixel image, with the following numbering:</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">4</td>
</tr>
</tbody>
</table>
<p>The pixels would be: (Y1Y2&ndash; Y3Y4&ndash;) (U1U2&ndash; U3U4&ndash;) (V1V2&ndash; V3V4&ndash;)<br>
Notice that the width is 2, therefore the row stride is 4 (empty data denoted by &lsquo;-&rsquo;). The image size would hence be calculated from the <code>default size</code> entry as $4 \times 2 + 4 \times 2 + 4 \times 2 = 24 \text{ components } = 24 \text{ bytes (8-bit components)}$</p>
<h6 id="yuy2">YUY2</h6>
<p>Packed 4:2:2 YUV</p>
<pre tabindex="0"><code>       +--+--+--+--+ +--+--+--+--+
       |Y0|U0|Y1|V0| |Y2|U2|Y3|V2| ...
       +--+--+--+--+ +--+--+--+--+

        Component 0: Y
          depth:           8
          pstride:         2
          offset:          0

        Component 1: U
          depth:           8
          offset:          1
          pstride:         4

        Component 2: V
          depth            8
          offset:          3
          pstride:         4

        Image
          default rstride: RU4 (width * 2)
          default size:    rstride (image) * height
</code></pre><p>Some notable differences:</p>
<ul>
<li>This is a packed format, and the placement of components is displayed on the top (also see example below)</li>
<li>Notice how pixel depth is still 8, even though this is a subsampled format. These two concepts are different</li>
<li>Because subsampling makes sense in a block of at least 2x2 pixels, each row (<code>rstride</code>) actually contains 2 rows of pixels, grouped by 2x2 blocks</li>
<li>Pay attention to the offset values as well, and how they correlate to the packed structure<br>
Again, for a 2x2 pixel image from before, the pixels would be: (Y1U1Y2V1) (Y3U2Y4V2)<br>
Notice how there are half as many chroma values from the previous format, and no filler data as everything is already aligned. Image size would be $4 \times 2 = 8 \text{ bytes}$!</li>
</ul>
<h6 id="i420">I420</h6>
<p>Planar 4:2:0 YUV</p>
<pre tabindex="0"><code>        Component 0: Y
          depth:           8
          pstride:         1
          default offset:  0
          default rstride: RU4 (width)
          default size:    rstride (component0) * RU2 (height)

        Component 1: U
          depth:           8
          pstride:         1
          default offset:  size (component0)
          default rstride: RU4 (RU2 (width) / 2)
          default size:    rstride (component1) * RU2 (height) / 2

        Component 2: V
          depth            8
          pstride:         1
          default offset:  offset (component1) + size (component1)
          default rstride: RU4 (RU2 (width) / 2)
          default size:    rstride (component2) * RU2 (height) / 2

        Image
          default size: size (component0) +
                        size (component1) +
                        size (component2)
</code></pre><p>Again, pixels are grouped into 2x2 blocks because of subsampling.<br>
For the 2x2 pixel image from before: (Y1Y2&ndash; Y3Y4&ndash;) (U1&mdash;) (V1&mdash;)<br>
The size is $8+4+4=16 \text{ bytes}$, which surprisingly, is two times the previous example because of alignment, even though there are half as much chroma data! This effect of course goes away in larger images, when alignment isn&rsquo;t a bottleneck.</p>
<h6 id="other-rgb">Other: RGB</h6>
<p>Even RGB has many different pixel formats. For example:</p>
<ul>
<li><strong>RGB24</strong>/<strong>RGB888</strong>: planar RGB with 8 depth</li>
<li><strong>RGB48</strong>: planar RGB with 16 depth</li>
<li><strong>RGB10</strong>: planar RGB with 10 depth<br>
They will not be covered.</li>
</ul>
<h4 id="endianness">Endianness</h4>
<p><a href="https://en.wikipedia.org/wiki/Endianness">Source</a><br>
Refers to how data gets stored as bytes. This concept comes up in some formats, like <code>I420_10LE</code>.<br>
Assume we have a 16-bit string that needs to be stored, e.g. <code>John</code>(ASCII).<br>
![[Pasted image 20231127160014.png]]<br>
on a big-endian machine, most significant bytes are stored in the smaller memory values (in the beginning)<br>
![[Pasted image 20231127160204.png]]<br>
On little-endian machines, the opposite happens<br>
![[Pasted image 20231127160233.png]]<br>
There are other such things as middle-endian, mixed-endian, etc. Exactly why this is done is unimportant at the moment, it&rsquo;s just good to know what it means.</p>
<h4 id="entropy-coding">Entropy Coding</h4>
<p>Source: ?<br>
<strong>Information Entropy</strong> is defined as the minimum number of bits that must be used to represent all the information contained in a dataset. For example, in a coin toss, you need 1 bit. In a double coin toss, you need 2 bits, etc.</p>
<p>An <strong>entropy encoder</strong> encodes data by reducing redundancy, without changing the entropy. For example, instead of 10 coin tosses being presented as 1111111111, they are represented as 10 1&rsquo;s. It is a method of <strong>lossless compression.</strong></p>
<p>Entropy compression relies on the fact that the distribution of data is skewed. <mark>If all values have equal probability of occurring, entropy compression is not effective</mark>. Entropy coding is usually the last step of compression, and all previous methods work towards introducing redundancy and increasing the entropy coding effectiveness.</p>
<h3 id="transforms-and-quantization">Transforms and Quantization</h3>
<p>These two steps are vital to pretty much every video and image compression algorithm. Transforms help locate less significant information in the image, and quantization discards this data while increasing redundancy. These stages prepare the image for [[#Entropy Coding]].</p>
<h4 id="transforms">Transforms</h4>
<p><em>Note: this section might have concepts covered before in the JPEG post, but some are repeated again because this is a good place for them to be.</em></p>
<p>A transform is a modification we make to data, converting it to another representation of itself. A simple example is the different representation of the number $3$. In base 2, it becomes $11$, which is just another representation. Another example was the [[#Color Model (Color Space)]] discussed before.</p>
<p>Transforms are used in image and video compression, because some transforms separate significant components from less significant ones.<br>
In the context of image compression, two types of transforms are generally used.</p>
<h5 id="block-based-transforms">Block Based Transforms</h5>
<p>These operate on blocks of $N \times N$ image or residual samples and hence the image is processed in units of a block. These have:</p>
<ul>
<li>Low memory requirements</li>
<li>Well suited to block-based MC (discussed later)</li>
<li>Suffer from blockiness (artifacts at the edges)<br>
Examples include:</li>
<li>KLT</li>
<li>SVD</li>
<li>DCT</li>
</ul>
<h6 id="discrete-cosine-transform">Discrete Cosine Transform</h6>
<p>DCT is a transform also used in compression methods like JPEG. To understand it, first we need to understand the Fourier Transform.</p>
<h6 id="what-is-the-fourier-transform-ft">What is the Fourier Transform (FT)</h6>
<p>The Fourier Transform is a transform that converts a function into a form that describes the frequencies present in the original function. FT is analogous to decomposing the continuous sound of a musical chord into terms of the intensity of its constituent pitches.</p>
<p>$$f(x)=\int_{-\infty}^{\infty}{\hat{f}}(\xi)\ e^{i2\pi\xi x},d\xi,\quad\forall x\in\mathbb{R}$$</p>
<p>This is the general form, but computers can&rsquo;t calculate this! Computers work with discrete samples of data, and they don&rsquo;t understand continuous math in calculations like we do. Also note that the $e^{\text{stuff}}$ value is another representation of sine and cosine.</p>
<p><strong>The Fourier Transform transforms a function $f(x)$ to a frequency domain, described only by sine and cosine (harmonic) functions.</strong></p>
<h6 id="discrete-time-fourier-transform-dtft">Discrete Time Fourier Transform (DTFT)</h6>
<p>An alternative representation of the Fourier Transform using an infinite sum instead of an integral:</p>
<p>$$
X_{2\pi }(\omega )=\sum _{n=-\infty }^{\infty }x[n],e^{-i\omega n}
$$</p>
<p>This is one step closer to something that can be processed by a computer, but the range is still infinite!</p>
<h6 id="fourier-transform-in-finite-values-dft-dct-and-dst">Fourier Transform in Finite Values (DFT, DCT and DST)</h6>
<p>In the context of computers, continuity doesn&rsquo;t really exist. Even the best microphones and cameras can only record a finite number of signals per unit of time.</p>
<p>An alternative method of describing the Fourier Transform is called the Discrete Fourier Transform (DFT), that works with a finite number of values:</p>
<p>$$
{\displaystyle X_{k}=\sum _{n=0}^{N-1}x_n\cdot e^{-{\frac{i2\pi }{N}}kn}}
$$</p>
<p>DFT is essentially equivalent to FT when $N \rightarrow \infty$ or when we have an infinite number of samples.<br>
<strong>When there are a finite number of samples, DFT ensures that a combination of harmonic functions can create the discrete values for us.</strong></p>
<p>For example, if we have a random set of 10 equally points in space, DFT can always give us a harmonic function that passes all of them.</p>
<p>Deviations like DCT and DST also exist. DFT gives a combination of sine/cosines (represented by the $e^{stuff}$) which can be used for complex numbers. DCT ensures only cosines, and DST only sines (real numbers). DCT and DST use half as many terms to describe the same values as DFT does, and each have their own properties and usecases. I&rsquo;m not sure about the specific properties, but for one, DCT&rsquo;s first term is $cos(0)=1$, while DST&rsquo;s is $sin(0)=0$, which makes representing some values easier in DCT (TODO: validate this).</p>
<h6 id="dct-in-action">DCT in Action</h6>
<p><a href="http://weitz.de/dct/">source</a> - most text here is quoted from this, I&rsquo;ve tried to give commentary that was helpful for myself, but reading the original is a <strong>prerequisite.</strong></p>
<p>As described before with DFT, <strong>DCT ensures that a combination of cosine functions can create any combination of discrete values for us.</strong> An example helps understand this.</p>
<h6 id="basic-example">Basic Example</h6>
<blockquote>
<p>We start by drawing the <a href="https://en.wikipedia.org/wiki/Trigonometric_functions">cosine</a> function between 0 and $\pi$. We then <em>sample</em> the function at certain points: We divide the interval from 0 to $\pi$ into three parts of equal size and take their centers, i.e. $x=\pi/6,$ $x=\pi/2$, and $x=5\pi/6$. Now we compute the values of the cosine function at these points. For image processing, we translate the results to different shades of gray, from black (−1) to white (1), so that 0 is medium gray.</p>
</blockquote>
<p>![[Pasted image 20231210130937.png]]</p>
<blockquote>
<p>We not only do this with the function $x=cos(1 \cdot x)$, but also with $x=cos(2 \cdot x)$ and with $x=cos(0 \cdot x)=1$. This gives us three vectors of three numbers, which we can interpret as three rectangles, each consisting of three gray blocks:<br>
![[Pasted image 20231210131100.png]]</p>
</blockquote>
<p>It is possible to prove that these three vectors:</p>
<ul>
<li>$(1,1,1)$</li>
<li>$(\frac{\sqrt{3}}{2},0,−\frac{\sqrt{3}}{2})$</li>
<li>$(0.5, -1, 0.5)$<br>
are linearly independent, which in turn implies that every vector of three numbers can be expressed as a linear combination of them!</li>
</ul>
<h6 id="generalized-to-pixel-arrays">Generalized to Pixel Arrays</h6>
<p>The same can be done in 2D. For a 3x3 block, this would contain values in</p>
<p>$$
z=\cos\left(ax\pi\right)\cos\left(by\pi\right)
$$</p>
<p>Where $z \in [-1,1], x,y \in [0,1]$ and $a,b \in \mathbb{N}, a,b \in [0,2]$.</p>
<p>Using the same coloring scheme as before, it will look like this:</p>
<blockquote>
<p>![[Pasted image 20231210132327.png]]</p>
</blockquote>
<p><a href="https://www.desmos.com/3d/c37e657662">Here is a 2D scheme in desmos</a>, when viewing the plot perpendicular to the XY axis, and changing the a, b values, the figures above make sense.</p>
<p>Any 3x3 pixel grid can be represented by a weighed sum of these 9 blocks. Here are some examples:</p>
<blockquote>
<p>![[Pasted image 20231210132506.png]]</p>
</blockquote>
<p>Notice how the more chaotic the pixel grid, the more coefficients are needed. The blue grids on the right show what weight is given to each pixel in the linearly independent pixel grid.<br>
High-frequency pixel grid values, like number 9, are less likely to have large values, and have a perceivable impact on the final result.</p>
<p>The same applies to other 2D grids. For example, a 16x16 macroblock used in H.264. The more chaotic a grid is (for example at borders, grainy parts, etc.), the more coefficients are needed to create a perceptual (unnoticeable) equivalent to it.</p>
<h6 id="how-dct-helps-in-compression">How DCT Helps in Compression</h6>
<p>By transforming a pixel array into its DCT form, it&rsquo;s possible to eliminate <strong>high frequency</strong> data that our eyes don&rsquo;t notice.<br>
Imagine you squint your eyes and look at something. The edges become blurry, and some colors bleed into one another. You are only seeing <strong>low frequency data, data that represents the general shape of the image.</strong><br>
High frequency data are seen in <strong>sharp boundaries</strong> and <strong>random</strong>, <strong>noisy</strong> and <strong>detailed</strong> parts of the image.</p>
<p>DCT gives us the tools needed to remove high frequency data. By removing the coefficients with a high order (in the bottom right in the example 3x3 block). This is great for natural images, as generally, natural images tend to be smooth, with no sudden changes.<br>
However, This method doesn&rsquo;t do too well in some scenarios:</p>
<ul>
<li>Animations, or images that are unnatural, sudden borders and change of colors</li>
<li>high-frequency or noisy images, like an image of leaves in the distance</li>
</ul>
<p>For these kinds of images, more coefficients are needed to recreate the original image with perceptual accuracy. Take a look at the <a href="http://weitz.de/dct/">previously mentioned source</a> to see some examples.</p>
<p>Exactly which coefficients to discard is another point of interest. This will be discussed in the [[#Quantization]] section, but just know that a lot of research has gone into which coefficients to discard and how, such that it is less noticeable by the human eye.</p>
<h6 id="integer-transform">Integer Transform</h6>
<p>Source: Chapter 7 of The H.264 Advanced Video Compression Standard (by Iain E. Richardson)</p>
<p>IT is used in H.264. It is derived from and is an <strong>approximation</strong> of DCT, designed to eliminate encoder/decoder mismatch, referred to as <strong>drift</strong>, and facilitate low complexity implementations.</p>
<p>One of the problems with DCT is that there are many irrational numbers present that can never be accurately represented with floating point numbers. They need to be approximated, which can cause problems:</p>
<ul>
<li>Different approximations can significantly change the output of the forward or inverse transforms, leading to mismatch between encoder/decoder (referred to as drift)</li>
<li>Even if the encoder and decoder meet certain criteria, there is still the problem of irrational numbers. Because the errors cumulate (due to the prediction that is based on previous frames), quality degradates over iterations.</li>
</ul>
<p>In H.264 and recent standards, the transform is designed to minimize computational complexity. This is done by using an integer approximation of the DCT. This reduces accuracy overall, but eliminates drift completely, because integers are interpreted the same in the encoder and decoder.</p>
<h5 id="image-based-transforms">Image Based Transforms</h5>
<p>These operate on an entire image or a section of it, known as a tile. They:</p>
<ul>
<li>Outperform block transforms for still image compression</li>
<li>Suffer from high memory requirements</li>
<li>Not well-suited for block-based MC<br>
Examples include:</li>
<li>DWT (wavelet)</li>
</ul>
<h6 id="discrete-wavelet-transform">Discrete Wavelet Transform</h6>
<p>Source and Reading Material: Chapter 3 of The H.264 Advanced Video Compression Standard (by Iain E. Richardson)</p>
<p>DWT or Wavelet works similar to DCT, by removing insignificant coefficients in a reversible transform. The wavelet transform, however, is applied to the whole image. It is computationally more expensive, but negates many of the negative effects of the block-based transform (like blocking artifacts)</p>
<p>I suggest reading the provided source for more information.</p>
<h4 id="quantization">Quantization</h4>
<p>Source: H.264 and MPEG-4 Video Compression (by Iain E. Richardson)</p>
<p>This step works hand in hand with the previous [[#Transforms]] step. The goal of quantization is removal of less significant data, which is a <strong>lossy</strong> process. The exact method is tuned to make perceptual losses minimal for our eyes.</p>
<p>To reduce the precision of the transformed data, A quantizer maps a signal with a range of values X to a quantized signal with a reduced range of values Y.</p>
<p>A scalar quantizer maps one sample of the input signal to one quantized output value and a vector quantizer maps a group of input samples, a ‘vector’, to a group of quantized values.</p>
<h5 id="scalar-quantization">Scalar Quantization</h5>
<p>The quantization method applies to a group of pixels, referred to as [[#Macroblocks]]. The overall method is to round to the nearest multiple of a number.</p>
<p>$$
FQ = \lfloor\frac{X}{QP}\rfloor \rightarrow Y = FQ \cdot QP
$$</p>
<p>Where $QP$ is the quantization &lsquo;step size&rsquo;, and $FQ$ is the forward quantization step (which is reversed with the inverse quantizer $IQ$). The following table includes some examples:</p>
<blockquote>
<p>![[Pasted image 20231210143908.png]]</p>
</blockquote>
<blockquote>
<p>If the step size is large, the range of quantized values is small and can therefore be efficiently represented and hence highly compressed during transmission, but the re-scaled values are a crude approximation to the original signal.</p>
<p>If the step size is small, the re-scaled values match the original signal more closely, but the larger range of quantized values reduces compression efficiency.</p>
<p>Quantization may be used to reduce the precision of image data after applying a transform such as the DCT or wavelet transform and to remove insignificant values such as near-zero DCT or wavelet coefficients. The output of a forward quantizer is therefore typically a ‘sparse’ array of quantized coefficients, mainly containing zeros.</p>
</blockquote>
<p>The example below should clear up what this means.</p>
<h6 id="scalar-quantization-example">Scalar Quantization Example</h6>
<p>Assume we have the transformed pixel data($P$) that describes the DCT coefficients for an 8x8 macroblock:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>92</td>
<td>3</td>
<td>-9</td>
<td>-7</td>
<td>3</td>
<td>-1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>-39</td>
<td>-58</td>
<td>12</td>
<td>17</td>
<td>-2</td>
<td>2</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>-84</td>
<td>62</td>
<td>1</td>
<td>-18</td>
<td>3</td>
<td>4</td>
<td>-5</td>
<td>5</td>
</tr>
<tr>
<td>-52</td>
<td>-36</td>
<td>-10</td>
<td>14</td>
<td>-10</td>
<td>4</td>
<td>-2</td>
<td>0</td>
</tr>
<tr>
<td>-86</td>
<td>-40</td>
<td>49</td>
<td>-7</td>
<td>17</td>
<td>-6</td>
<td>-2</td>
<td>5</td>
</tr>
<tr>
<td>-62</td>
<td>65</td>
<td>-12</td>
<td>-2</td>
<td>3</td>
<td>-8</td>
<td>-2</td>
<td>0</td>
</tr>
<tr>
<td>-17</td>
<td>14</td>
<td>-36</td>
<td>17</td>
<td>-11</td>
<td>3</td>
<td>3</td>
<td>-1</td>
</tr>
<tr>
<td>-54</td>
<td>32</td>
<td>-9</td>
<td>-9</td>
<td>22</td>
<td>0</td>
<td>1</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>As we saw in the [[#DCT in Action]] section, the coefficients on the bottom right usually have less impact on the overall picture, as seen by their small values.</p>
<p>Next, we define a <strong>quantization table</strong> ($T$). It is also a 8x8 table that describes the QP values for each values in the table:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>5</td>
<td>7</td>
<td>9</td>
<td>11</td>
<td>13</td>
<td>15</td>
<td>17</td>
</tr>
<tr>
<td>5</td>
<td>7</td>
<td>9</td>
<td>11</td>
<td>13</td>
<td>15</td>
<td>17</td>
<td>19</td>
</tr>
<tr>
<td>7</td>
<td>9</td>
<td>11</td>
<td>13</td>
<td>15</td>
<td>17</td>
<td>19</td>
<td>21</td>
</tr>
<tr>
<td>9</td>
<td>11</td>
<td>13</td>
<td>15</td>
<td>17</td>
<td>19</td>
<td>21</td>
<td>23</td>
</tr>
<tr>
<td>11</td>
<td>13</td>
<td>15</td>
<td>17</td>
<td>19</td>
<td>21</td>
<td>23</td>
<td>25</td>
</tr>
<tr>
<td>13</td>
<td>15</td>
<td>17</td>
<td>19</td>
<td>21</td>
<td>23</td>
<td>25</td>
<td>27</td>
</tr>
<tr>
<td>15</td>
<td>17</td>
<td>19</td>
<td>21</td>
<td>23</td>
<td>25</td>
<td>27</td>
<td>29</td>
</tr>
<tr>
<td>17</td>
<td>19</td>
<td>21</td>
<td>23</td>
<td>25</td>
<td>27</td>
<td>29</td>
<td>31</td>
</tr>
</tbody>
</table>
<p>Quantization tables are defined such that perceptual quality is maintained as much as possible. The higher values imply more data loss. Using the formula described before on the first row and column of the pixel array:</p>
<p>$$
Q_{11} = \lfloor \frac{92}{3} \rfloor \times 3 = 90
$$</p>
<p>Likewise, for the bottom right value in the table:</p>
<p>$$
Q_{88} = \lfloor \frac{3}{31} \rfloor \times 31 = 0
$$</p>
<p>This is calculated for all the values, and this is the result:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>90</td>
<td>0</td>
<td>-7</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-35</td>
<td>-56</td>
<td>9</td>
<td>11</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-84</td>
<td>54</td>
<td>0</td>
<td>-13</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-45</td>
<td>-33</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-77</td>
<td>-39</td>
<td>45</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-52</td>
<td>60</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-15</td>
<td>0</td>
<td>-19</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>-51</td>
<td>19</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The higher values of $Q$ in the bottom right cause the final values to be zero in those ranges, allowing data compression by discarding information.</p>
<h6 id="where-does-the-quantization-table-come-from">Where Does the Quantization Table Come From</h6>
<p>There are many ways to choose a quantization table, and each compression level has its own quantization table. The bottom right values are usually very high, though, since high-frequency data is usually discarded.<br>
Finding the exact quantization table can be done in different ways:</p>
<ul>
<li>Measuring the mathematical error found between an input image and its output image after it has been decompressed, trying to determine an acceptable level of error.</li>
<li>Using other quality assessment methods, such as methods reliant on machine learning. These try to achieve the same visual quality by tuning the quantization values.</li>
</ul>
<h5 id="vector-quantization">Vector Quantization</h5>
<p>Similar to scalar quantization, but instead of applying it to every value, we can apply it to a vector instead. A <strong>codebook</strong> contains a lot of different vectors (2D grid of values), the encoder finds the closest vector and transmits an index, instead of all the values in the grid.<br>
![[Pasted image 20231210145946.png]]</p>
<p>This is used in compression of <strong>motion vectors</strong>, which is outside the scope of this document but will be covered in a future post.</p>
<h3 id="general-video-compression-concepts">General Video Compression Concepts</h3>
<p>Source: H.264 and MPEG-4 Video Compression (by Iain E. Richardson)<br>
Source: The H.264 Advanced Video Compression Standard (by Iain E. Richardson)<br>
Source: Fundamentals of Multimedia (by Ze-Nian Li, Mark S. Drew, Jiangchuan Liu)</p>
<p>The way a video stream is transmitted is important to know, even if just on a basic level. These concepts can be seen most specifications today.<br>
Generally, a video stream is segmented into different structures. This makes sense for many reasons, such as parallel processing and adaptive quality changing and random access decoding to name a few.</p>
<p>Different specifications define different concepts, so the concepts described here are mostly used by codecs developed by ITU-T/IEC (<em>even though I said these are &ldquo;general&rdquo; concepts, they might not exist with these exact definitions in other codecs :D</em>)</p>
<p>A video is comprised of:</p>
<ul>
<li>Group of Picture (GOP): comprised of pictures</li>
<li>Picture: comprised of slices</li>
<li>Slice: comprised of macroblocks</li>
<li>Macroblock: comprised of samples</li>
<li>Sample: elementary color components (also see [[#Color Model (Color Space)]])</li>
</ul>
<h4 id="samples">Samples</h4>
<p>The smallest unit, represented by the single luma, or two chroma components of a single pixel.</p>
<h4 id="blocks">Blocks</h4>
<p>Or Pixels; an aggregation of samples</p>
<h4 id="macroblocks">Macroblocks</h4>
<p>Unit of processing used in many standards like MPEG-4, H.264 and JPEG. Instead of working on the whole image at once, the image is first split into smaller sections called macroblocks. In JPEG, for example, these are 8x8. In modern video compression standards, the size can vary based on the contents.</p>
<p>Some concepts defined before, like [[#Transforms]], affect these macroblocks.</p>
<p>Because of this segmented approach, it&rsquo;s possible that the boundary between macroblocks may look unnatural after decompression (see [[#Blocking]]). To avoid this, algorithms like <strong>deblocking</strong> or <strong>loop filtering</strong> can be used at the decoder side.</p>
<h5 id="sub-macroblocks">Sub Macroblocks</h5>
<p>Described in specifications like H.264. A 16 × 16-pixel region of the source frame is represented by 256 luminance samples arranged in four 8 × 8-sample blocks, 64 red chrominance samples in one 8 × 8 block and 64 blue chrominance samples in one 8 × 8 block, giving a total of six 8 × 8 blocks. An H.264/AVC codec processes each video frame in units of a macroblock.<br>
![[Pasted image 20231220131029.png]]</p>
<h4 id="slices">Slices</h4>
<p><em>Note: The H.264 specification is used as a reference for this, and the exact details should not be taken as fact for all specifications.</em></p>
<p>Macroblocks are collected into slices. The slice is a standardized component and the decoder knows that if it loses synchronization at the macroblock level, it can reconstruct the picture starting at the next slice boundary. This helps the decoder in the receiver respond elegantly to dropouts in the signal and only a portion of the frame is then lost. Error recovery replaces the missing area of the screen with corresponding macroblocks from a previous frame in the playback sequence.<br>
In short, <strong>the slice structure provides the lowest layer mechanism for resynchronization in case of transmission bit errors.</strong><br>
![[Pasted image 20231220131434.png]]<br>
Although a slice contains macroblocks or macroblock pairs that are consecutive in the raster scan within a slice group, these macroblocks or macroblock pairs are not necessarily consecutive in the raster scan within the picture.</p>
<p>Specifically, H.264 describes different slice types. The unique SI and SP slices enable (among other things) efficient switching between video streams and efficient random access for video decoders.<br>
![[Pasted image 20231220163635.png]]</p>
<h5 id="intra-coded-slice-i-slice">Intra-coded Slice (I-Slice)</h5>
<p>Slice that is decoded using only [[#Intra Prediction]] and is not an [[#Switching I Slice (SI-Slice)]].</p>
<h5 id="predictive-slice-p-slice">Predictive Slice (P-Slice)</h5>
<p>Slice that may be decoded using [[#Intra Prediction]] or [[#Inter Prediction]] and is not an [[#Switching P Slice (SP-Slice)]]. Uses at most one motion vector and reference index to predict the sample values of each macroblock. Up to 16 references.</p>
<h5 id="bipredictive-slice-b-slice">Bipredictive Slice (B-Slice)</h5>
<p>Slice that may be decoded using [[#Intra Prediction]] or [[#Inter Prediction]] using at most two motion vectors and reference indices to predict the sample values of each macroblock. Up to 16 references.</p>
<h5 id="switching-i-slice-si-slice">Switching I Slice (SI-Slice)</h5>
<p>Slice that is coded using [[#Intra Prediction]] and using quantization of the prediction samples. An SI slice can be coded such that its decoded samples can be constructed identically to an [[#Switching P Slice (SP-Slice)]].</p>
<h5 id="switching-p-slice-sp-slice">Switching P Slice (SP-Slice)</h5>
<p>Slice that may be decoded using [[#Intra Prediction]] or [[#Inter Prediction]] with quantization of the prediction samples using at most one motion vector and reference index to predict the sample values of each macroblock. An SP slice can be coded such that its decoded samples can be constructed identically to another SP slice or an SI slice.</p>
<h5 id="instantaneous-decoding-refresh-slice-idr-slice">Instantaneous Decoding Refresh Slice (IDR-Slice)</h5>
<p>A special type of [[#Intra-coded Slice (I-Slice)]] that causes the decoding process to mark all reference pictures as &ldquo;unused for reference&rdquo; immediately after the decoding of the IDR picture. All coded pictures that follow an IDR picture in decoding order can be decoded without inter prediction from any picture before it.</p>
<p>The IDR slice simply tells the decoder to discard any frame history or buffer that it has stored so far, and act though there have been no slices before the IDR slice.<br>
We can infer some things from this:</p>
<ul>
<li>Normally, an [[#Intra-coded Slice (I-Slice)]] can be a reference for frames before it. For example, a P-frame can technically reference I-frames that come after it.</li>
<li>An IDR slice must also signal the start of an IDR [[#Pictures (Frames or Fields)|picture]]. <strong>Since no backward referencing is allowed, the whole frame following an IDR slice needs to be composed of I-Slices.</strong></li>
<li>It also explains why the starting slice in the decoder should be (and is) an IDR slice. This way, the decoder treats an IDR slice exactly the same as if the slice decoding process has JUST started, by discarding all slices before it.</li>
</ul>
<h4 id="pictures-frames-or-fields">Pictures (Frames or Fields)</h4>
<p>A frame contains an array of luma samples in monochrome format or an array of luma samples and two corresponding arrays of chroma samples in 4:2:0, 4:2:2, and 4:4:4 color format.</p>
<p>A video signal may be sampled as a series of complete frames, <strong>progressive</strong> sampling, or as a sequence of interlaced fields, <strong>interlaced</strong> sampling. In an interlaced video sequence, half of the data in a frame, one field, is typically sampled at each temporal sampling interval. A field may consist of either the odd-numbered or even-numbered lines within a complete video frame and an interlaced video sequence.</p>
<h5 id="frame-types">Frame Types</h5>
<p>Similarly to what was described in [[#Slices]], frames can also have different types. For example, for H.264, here are what the common frame types refer to:</p>
<h6 id="intra-coded-frame-i-frame">Intra-coded Frame (I-Frame)</h6>
<p>Composed only of [[#Intra-coded Slice (I-Slice)]]s.</p>
<h6 id="predictive-frame-p-frame">Predictive Frame (P-Frame)</h6>
<p>Composed only of [[#Intra-coded Slice (I-Slice)]] or [[#Predictive Slice (P-Slice)]]s.</p>
<h6 id="bipredictive-frame-b-frame">Bipredictive Frame (B-Frame)</h6>
<p>Composed only of [[#Intra-coded Slice (I-Slice)]], [[#Predictive Slice (P-Slice)]], or [[#Bipredictive Slice (B-Slice)]]s.</p>
<h5 id="field-types">Field Types</h5>
<h6 id="progressive">Progressive</h6>
<p>The conventional method and used by today&rsquo;s digital devices.</p>
<h6 id="interlaced">Interlaced</h6>
<p>Basically, two different frames are interleaved, doubling temporal resolution at the cost of halving spacial resolution (height) of each frame.<br>
![[Pasted image 20231202143724.png]]</p>
<p>Interlacing is a result of an issue with older displays (see <a href="https://www.youtube.com/watch?v=5eu_KjKsnpM">Cap D&rsquo;s Vid</a>), where the raster scanning process took very long. Interlacing isn&rsquo;t used today, because it doesn&rsquo;t have benefits anymore, and leaves [[#Interlacing Artifacts]] when converted to progressive.</p>
<h4 id="group-of-pictures-gop">Group of Pictures (GOP)</h4>
<p>A grouping of pictures or frames; each GOP always starts with an I-frame. This type isn&rsquo;t specified in the official H.264 specifications, because it can be inferred from the presence of IDR frames and isn&rsquo;t a new concept by any means.</p>
<p>There are two types of GOP structures:</p>
<h5 id="open-gop">Open GOP</h5>
<p>Achievable by never placing an [[#Instantaneous Decoding Refresh Slice (IDR-Slice)|IDR Slice]] or frame after the first one. This means situations like this are <strong>always</strong> possible:<br>
![[Pasted image 20231220174412.png]]<br>
This makes it impossible to find a suitable point at which to break a sequence up, which means no possibility of seeking without significant artifacts appearing until the back-references disappear. It <em>can</em> provide better compression, though.</p>
<h5 id="closed-gop">Closed GOP</h5>
<p>Has no external dependencies to other GOPs, it can be achieved by placing an [[#Instantaneous Decoding Refresh Slice (IDR-Slice)|IDR Slice]] at the beginning of each GOP. Makes more sense for streamed content, as playback can be started midway without artifacts.<br>
![[Pasted image 20231219130405.png]]</p>
<h3 id="compression-artifacts">Compression Artifacts</h3>
<ul>
<li>Main Source: Characterizing Perceptual Artifacts in Compressed Video Streams (by Kai Zeng, Tiesong Zhao, Abdul Rehman and Zhou Wang)</li>
<li><a href="https://guide.encode.moe/encoding/video-artifacts.html">Source</a></li>
<li>WikiPeida</li>
</ul>
<p>In this section, some artifacts of compression are covered. They help us correlate problems in the decoded image with the part of the encoder that caused it.</p>
<p>![[Pasted image 20231210110107.png]]</p>
<h4 id="spatial-artifacts">Spatial Artifacts</h4>
<p>Without attention to previous/next frames. Classification is <strong>subjective</strong>, based on their visual appearance.<br>
Can be due to:</p>
<ul>
<li>Partitioning</li>
<li>Quantization</li>
<li>De-blocking Filter (perceptual blurring with a adaptive low-pass filter)</li>
</ul>
<h5 id="blurring">Blurring</h5>
<p>Can happen both at the quantization and de-blocking steps.<br>
![[Pasted image 20231210111026.png]]</p>
<p>The image on the left is the original, while the ones on the right are coded with H.264. In the middle image, de-blocking is turned off. Here, macroblock boundaries are sharp, especially at high-frequency locations like the boundary between two objects. Blurring is still visible within the macroblocks, because of quantization.<br>
The image on the right has de-blocking turned on. Macroblock boundaries are far less visible, but the overall image is also much more blurry.</p>
<h5 id="blocking">Blocking</h5>
<p>Very common, especially in JPEG where de-blocking isn&rsquo;t common. Blocking happens due to [[#Block Based Transforms]] like DCT.</p>
<p>Blocking has three categories:</p>
<h6 id="mosaic-effect">Mosaic Effect</h6>
<p>The Mosaic effect happens at low-energy regions, where the macroblock quantization yields the same DCT values for all pixels. It is very visible to the human eye, though, because of the surrounding blocks.<br>
![[Pasted image 20231210112239.png]]</p>
<p>A variation, Banding, happens in smooth gradients. It is very observable in Anime. Banding can also be due to insufficient bit depth when sampling a continuous gradation of color tone. It can be remedied with methods like dithering.<br>
![[Pasted image 20231210113415.png]]</p>
<h6 id="staircase-effect">Staircase Effect</h6>
<p>Typically happens along a diagonal line. Can be due to insufficient sampling rates, or low spatial resolution.<br>
![[Pasted image 20231210112217.png]]<br>
![[Pasted image 20231210112400.png]]</p>
<h6 id="false-edge">False Edge</h6>
<p>Fake edge that appears near a true one. This can be due to motion estimation, where the previous frame itself had blocking, so the inter-predicted frame accumulates that blocking effect to create many edges near a true one.<br>
![[Pasted image 20231210112702.png]]</p>
<h5 id="ringing">Ringing</h5>
<ul>
<li>WikiPedia<br>
Due to quantization, high frequency data, such as sharp transitions between objects, can see wave-like or ripple structures at the edges.<br>
![[Pasted image 20231210113550.png]]</li>
</ul>
<h5 id="basis-pattern">Basis Pattern</h5>
<p>The origin is similar to [[#Ringing]] effect, but is not restricted to sharp edges. Happens at medium-energy regions like grass, and due to loss of texture from the block transform.<br>
![[Pasted image 20231210114107.png]]</p>
<h5 id="color-bleeding">Color Bleeding</h5>
<p>During subsampling, gamma-corrected signals like $Y\prime C_rC_b$ have an issue where chroma errors &ldquo;bleed&rdquo; into luma. In those signals, a low chroma actually makes a color appear less bright than one with equivalent luma. As a result, when a saturated color blends with an unsaturated or complementary color, a loss of luminance occurs at the border. There are proposed corrections for this (see <a href="https://en.wikipedia.org/wiki/Chroma_subsampling#Gamma_luminance_error">here</a>).<br>
![[Pasted image 20231210114706.png]]<br>
![[Pasted image 20231210114727.png]]</p>
<h4 id="temporal-artifacts">Temporal Artifacts</h4>
<p>Not observed in a paused frame, but during playback. These need more attention, because quality assessment models such as SSIM, PSNR, etc. only work on static frames, and not consecutive ones. So they are useless for these kind of artifacts!</p>
<h5 id="flickering">Flickering</h5>
<p>Frequent luminance or chrominance changes along a temporal dimension. Can be classified based on the frequency and spatial locations where they occur:</p>
<h6 id="mosquito-noise">Mosquito Noise</h6>
<p>Can happen when [[#Ringing]] or motion prediction errors are combined with object motion, especially near object boundaries. It gives the impression of mosquitos moving around a region, and is very noticeable because of this.<br>
![[Pasted image 20231210115356.png]]</p>
<h6 id="coarse-granularity-flickering">Coarse-Granularity Flickering</h6>
<p>Sudden luminance changes in large spatial regions that could extend to the entire video frame. This is caused most likely because of the use of closed GoP structures. Let me explain:</p>
<p>For example, in a closed I-P-P-…-P GoP structure, the first frame, called I-frame is coded independently using intra-coding method, while all other frames, called P-frames, are coded after predictions from the previous frames. The length of a GoP is limited, and thus when a new GoP starts, there is no dependency between the last P-frame in the previous GoP and the I-frame in the current GoP, and thus sudden luminance change is likely to be observed, especially when these two frames are about the same scene.</p>
<p>Changing GOP size can help reduce this effect, while employing I-frames only at scene changes completely negates it.</p>
<h6 id="fine-granularity-flickering">Fine-Granularity Flickering</h6>
<p>Observed in large low/mid energy regions with significant blocking effect and slow motion. This causes spatial artifacts like [[#Blocking]], that when combined with changing DC between frames, can cause flashing blocks at high frequency.</p>
<p>As opposed to Coarse-Granularity flickering, these happen frame-by-frame as opposed to GOP-by-GOP.</p>
<h5 id="jerkiness">Jerkiness</h5>
<p>Isn&rsquo;t really considered a compression artifact.</p>
<p>Can happen due to low temporal resolution (low fps), when smooth motion is seen as sudden jerking. Can happen due to bandwidth constrains.</p>
<p>Modern encoders can use multi-layer resolutions (fine-resolution layers are transmitted separately), and when the fine resolution layers are not transmitted properly, these can be seen.</p>
<p>Jerkiness can also happen due to upscaling, especially upscaling from 24 to 30 fps, where some generated frames can have more motion than others.</p>
<h5 id="floating">Floating</h5>
<p>Appearance of motion in only one part of a region, when the whole region should be moving together. Happens when the encoder skips motion compensated prediction residue; meaning some regions get skipped, and the other regions look like they are floating above the skipped regions.<br>
#todo look into Skip mode in encoders, as they cause this!</p>
<p>Visually, these regions create a strong perceptual illusion as if they were floating on top of the surrounding background.</p>
<p>Can be classified based on where they occur:</p>
<h6 id="texture-floating--ghosting">Texture Floating / Ghosting</h6>
<p>Happens at large, mid-energy regions. For example, dense tree structures where texture floating is detected in consecutive frames:<br>
![[Pasted image 20231210121329.png]]</p>
<p>Mathematically, the use of Skip mode makes sense for the encoder, but visually, it is very jarring. Skip mode can happen when there is:</p>
<ul>
<li>Global camera motion, like translation, rotation or zooming</li>
<li>Mid-energy regions</li>
</ul>
<h6 id="edge-neighborhood-floating--stationary-area-temporal-fluctuations">Edge Neighborhood Floating / Stationary Area Temporal Fluctuations</h6>
<p>Can happen at the edge of a moving, and a static object. Happens again due to Skip mode, which simply copies the regions surrounding object boundaries from one frame to another without any further update in image details.</p>
<p>Happens without global motion, and looks like there exists a wrapped package surrounding and moving together with the object boundaries.</p>
<h4 id="other-image-artifacts">Other Image Artifacts</h4>
<p>There are many different types of <a href="https://en.wikipedia.org/wiki/Visual_artifact">visual</a> and <a href="https://en.wikipedia.org/wiki/Digital_artifact">digital</a> artifacts, and not all of them can be covered. I was curious about some, which I&rsquo;ll cover here.</p>
<h4 id="interlacing-artifacts">Interlacing Artifacts</h4>
<p>Seen in older footage, and occurs in regions of high motion, because of the introduction of future frames, to increase temporal dimension.<br>
![[Pasted image 20231220152115.png]]</p>
<p>De-interlacing methods aim to fix this issue, for example by interpolating between the two versions, or by halving the height (removing the interlaced field).</p>
<h4 id="aliasing">Aliasing</h4>
<p>At its core, image aliasing results from an imbalance between an image’s frequency content and the sampling rate deployed by digital imaging technologies. Once these image characteristics surpass the half-of-sampling-rate limit, under-sampling occurs — an event directly tied to image aliasing.</p>
<p>When the image details or patterns exceed this half-of-sampling-rate threshold, we witness a phenomenon called under-sampling. This is where image aliasing gets its roots. Under-sampling results in the lack of necessary data to replicate high-frequency components of the image, thus leading to the misinterpretation of these components and, eventually, the creation of the aliasing effect.</p>
<p>![[Pasted image 20231210123203.png]]</p>
<p>Aliasing also has a similar concept (different source) in computer graphics, and refers to when smooth, diagonal lines look like steps:<br>
![[Pasted image 20231210123315.png]]</p>
<h3 id="image-quality-assessment">Image Quality Assessment</h3>
<p>How are encoded images or videos assessed for quality? There are some methods commonly used.</p>
<h4 id="psnr">PSNR</h4>
<p>(Peak Signal to Noise Ratio): indicates absolute error between the original and impaired image. Very fast, but doesn&rsquo;t do well in subjective human comparisons</p>
<h4 id="ssim">SSIM</h4>
<p>(Structural Similarity Index Measure): perception-based model that considers image degradation as perceived change in structural information.</p>
<h4 id="ms-ssim">MS-SSIM</h4>
<p>(Multiscale SSIM): improved version of SSIM</p>
<h4 id="vmaf">VMAF</h4>
<p>(Video Multimethod Assessment Fusion): objective full-reference video quality metric developed by Netflix in cooperation with the University of Southern California. VMAF is trained on real viewing data, and has different models for different use cases</p>
<ul>
<li>Default model: based on the assumption that the viewers sit in front of a 1080p display in a living room-like environment with the viewing distance of 3x the screen height (3H)</li>
<li>Phone model: To accurately reflect how a viewer perceives quality on a phone. In particular, due to smaller screen size and longer viewing distance relative to the screen height (&gt;3H), viewers perceive high-quality videos with smaller noticeable differences</li>
<li>4k model: predicts the subjective quality of video displayed on a 4K TV and viewed from a distance of 1.5H</li>
</ul>
<h4 id="example">Example</h4>
<p>The original is blurred twice to create <code>dist-1</code> and <code>dist-2</code>. In <code>dist-3</code>, only the background is blurred and colors are more saturated.<br>
![[Screenshot 2023-12-06 at 11.55.27.png]]</p>
<table>
<thead>
<tr>
<th style="text-align:center">score/image</th>
<th>dist-1</th>
<th>dist-2</th>
<th>dist-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">psnr</td>
<td>38.56</td>
<td> 33.45</td>
<td>26.98</td>
</tr>
<tr>
<td style="text-align:center">ssim</td>
<td>0.92</td>
<td>0.84</td>
<td>0.77</td>
</tr>
<tr>
<td style="text-align:center">ms-ssim</td>
<td>0.94</td>
<td>0.80</td>
<td>0.74</td>
</tr>
<tr>
<td style="text-align:center">vmaf (default)</td>
<td>42.17</td>
<td>23.56</td>
<td>40.58</td>
</tr>
</tbody>
</table>
<p>VMAF is trained on real viewer data. Even though mathematics might show dist-3 is less identical to the original, subjective viewers give more important to the face and subject, which is clearer in dist-3.</p>
<h3 id="sources">Sources</h3>
<p>Most sources have been credited where they have been used. If any source isn&rsquo;t credited, please reach out to me.</p>
</div>
				
				<footer class="entry__footer">
					
<div class="entry__tags">
			<a class="entry__tag btn" href="/tags/tag1/">Tag1</a>
			<a class="entry__tag btn" href="/tags/tag2/">Tag2</a>
</div>
					
<div class="entry__share share">
	<a class="share__link btn" title="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=%2f%2flocalhost%3a1313%2feducational%2ftest%2f" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on Facebook', 'width=800,height=600,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Facebook" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M330 512V322h64l9-74h-73v-47c0-22 6-36 37-36h39V99c-7-1-30-3-57-3-57 0-95 34-95 98v54h-64v74h64v190z"/></svg>
	</a>
	<a class="share__link btn" title="Share on Twitter" href="https://twitter.com/intent/tweet/?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&amp;text=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on Twitter', 'width=800,height=450,resizable=yes,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Twitter" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
	</a>
	<a class="share__link btn" title="Share on Reddit" href="https://www.reddit.com/submit?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&amp;title=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on Reddit', 'width=832,height=624,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Reddit" role="img" width="32" height="32" viewBox="0 0 512 512"><path fill-rule="evenodd" d="M375 146a32 32 0 1 0-29-46l-65-13c-5-1-9 2-10 6l-22 97c-45 1-85 15-113 36a42 42 0 1 0-45 69l-1 12c0 65 74 117 166 117s166-52 166-117l-1-11a42 42 0 1 0-44-69c-28-21-67-35-111-37l19-86 58 13a32 32 0 0 0 32 29zM190 353c2-1 4 0 5 1 15 11 38 18 61 18s46-6 61-18a7 7 0 0 1 8 10c-18 14-44 21-69 21-25-1-51-7-69-21a6 6 0 0 1 3-11zm23-44a31 31 0 1 1-44-44 31 31 0 0 1 44 44zm130 0a31 31 0 1 0-44-44 31 31 0 0 0 44 44z"/></svg>
	</a>
	<a class="share__link btn" title="Share on Telegram" href="https://t.me/share/url?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&amp;title=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on Telegram', 'width=800,height=600,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Telegram" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M199 404c-11 0-10-4-13-14l-32-105 245-144"/><path d="M199 404c7 0 11-4 16-8l45-43-56-34"/><path d="M204 319l135 99c14 9 26 4 30-14l55-258c5-22-9-32-24-25L79 245c-21 8-21 21-4 26l83 26 190-121c9-5 17-3 11 4"/></svg>
	</a>
	<a class="share__link btn" title="Share on LinkedIn" href="https://www.linkedin.com/shareArticle?mini=true&url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&title=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on LinkedIn', 'width=640,height=480,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="LinkedIn" role="img" width="32" height="32" viewBox="0 0 512 512"><circle cx="142" cy="138" r="37"/><path stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
	</a>
	<a class="share__link btn" title="Share on VK" href="https://vk.com/share.php?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Share on VK', 'width=640,height=480,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="VK" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M274 363c5-1 14-3 14-15 0 0-1-30 13-34s32 29 51 42c14 9 25 8 25 8l51-1s26-2 14-23c-1-2-9-15-39-42-31-30-26-25 11-76 23-31 33-50 30-57-4-7-20-6-20-6h-57c-6 0-9 1-12 6 0 0-9 25-21 45-25 43-35 45-40 42-9-5-7-24-7-37 0-45 7-61-13-65-13-2-59-4-73 3-7 4-11 11-8 12 3 0 12 1 17 7 8 13 9 75-2 81-15 11-53-62-62-86-2-6-5-7-12-9H79c-6 0-15 1-11 13 27 56 83 193 184 192z"/></svg>
	</a>
	<a class="share__link btn" title="Save to Pocket" href="https://getpocket.com/edit?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&amp;title=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Save to Pocket', 'width=480,height=320,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Pocket" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M388.8 88.9H123.2A47.4 47.4 0 0 0 76 136.5v131.9c0 2.4.2 4.8.5 7.2a101.8 101.8 0 0 0-.5 10.6c0 75.6 80.6 137 180 137s180-61.4 180-137c0-3.6-.2-7.1-.5-10.6.3-2.4.5-4.8.5-7.2v-132A47.4 47.4 0 0 0 388.8 89zm-22.4 132.6l-93 93c-4.7 4.6-11 7-17.1 7a23.8 23.8 0 0 1-17.7-7l-93-93a24 24 0 0 1 33.8-33.8l76.6 76.5 76.6-76.5a24 24 0 0 1 33.8 33.8z"/></svg>
	</a>
	<a class="share__link btn" title="Save to Pinterest" href="https://pinterest.com/pin/create/button/?url=%2f%2flocalhost%3a1313%2feducational%2ftest%2f&description=Test" target="_blank" rel="noopener noreferrer" onclick="window.open(this.href, 'Save to Pocket', 'width=800,height=720,toolbar=0,status=0'); return false">
		<svg class="share__icon" aria-label="Pinterest" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="m265 65c-104 0-157 75-157 138 0 37 14 71 45 83 5 2 10 0 12-5l3-18c2-6 1-7-2-12-9-11-15-24-15-43 0-56 41-106 108-106 60 0 92 37 92 85 0 64-28 116-70 116-23 0-40-18-34-42 6-27 19-57 19-77 0-18-9-34-30-34-24 0-42 25-42 58 0 20 7 34 7 34l-29 120a249 249 0 0 0 2 86l3-1c2-3 31-37 40-72l16-61c7 15 29 28 53 28 71 0 119-64 119-151 0-66-56-126-140-126z"/></svg>
	</a>
</div>
				</footer>
				
			</article>
		</div>
	</main>
	
<div class="authorbox block">
	<div class="author">
		<div class="author__body">
			<div class="author__name">
				
			</div>
		</div>
	</div>
</div>
	



	

	</div>
	<footer class="footer">
<div class="footer__social social">
		<a class="social__link" target="_blank" rel="noopener noreferrer" href="mailto:seyed.alireza.miryazdi@gmail.com">
			<svg class="social__icon" aria-label="Email" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M299 268l124 106c-4 4-10 7-17 7H106c-7 0-13-3-17-7l124-106 43 38 43-38zm-43 13L89 138c4-4 10-7 17-7h300c7 0 13 3 17 7L256 281zm54-23l121-105v208L310 258zM81 153l121 105L81 361V153z"/></svg>
		</a>
		<a class="social__link" target="_blank" rel="noopener noreferrer" href="https://t.me/XosrovX">
			<svg class="social__icon" aria-label="Telegram" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M199 404c-11 0-10-4-13-14l-32-105 245-144"/><path d="M199 404c7 0 11-4 16-8l45-43-56-34"/><path d="M204 319l135 99c14 9 26 4 30-14l55-258c5-22-9-32-24-25L79 245c-21 8-21 21-4 26l83 26 190-121c9-5 17-3 11 4"/></svg>
		</a>
		<a class="social__link" target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/alireza-miryazdi">
			<svg class="social__icon" aria-label="LinkedIn" role="img" width="32" height="32" viewBox="0 0 512 512"><circle cx="142" cy="138" r="37"/><path stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
		</a>
		<a class="social__link" target="_blank" rel="noopener noreferrer" href="https://github.com/Xosrov">
			<svg class="social__icon" aria-label="Github" role="img" width="32" height="32" viewBox="0 0 512 512"><path d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
		</a>
</div>
	<div class="footer__copyright">© 2024 Alireza&#39;s Mindmap. <span class="footer__copyright-credits">Powered by <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/vimux/binario" rel="nofollow noopener" target="_blank">Binario</a> theme.</span></div>
</footer>
<script src="/js/menu.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
    }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
</script>

</body>
</html>